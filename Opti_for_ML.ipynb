{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Opti for ML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97otrLqBVrwO"
      },
      "source": [
        "# Imports and data uploa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlmW0d3jO3z-"
      },
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from google.colab import files\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0Z2aWD-dWd3b",
        "outputId": "7445b22b-a698-496e-c46b-a9ccd5aa491a"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-97889392-8b89-4b29-b142-e47ba7e1d94f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-97889392-8b89-4b29-b142-e47ba7e1d94f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving heart.csv to heart.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JHar_IeZJ45"
      },
      "source": [
        "heart = pd.read_csv(\"heart.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxdHe7gVyrR"
      },
      "source": [
        "# Data wrangling and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hL6ofQuJZQy5",
        "outputId": "e46eff7c-8abe-4bdd-eef3-96ddcbb0fe3d"
      },
      "source": [
        "heart.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trtbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalachh</th>\n",
              "      <th>exng</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slp</th>\n",
              "      <th>caa</th>\n",
              "      <th>thall</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trtbps  chol  fbs  ...  exng  oldpeak  slp  caa  thall  output\n",
              "0   63    1   3     145   233    1  ...     0      2.3    0    0      1       1\n",
              "1   37    1   2     130   250    0  ...     0      3.5    0    0      2       1\n",
              "2   41    0   1     130   204    0  ...     0      1.4    2    0      2       1\n",
              "3   56    1   1     120   236    0  ...     0      0.8    2    0      2       1\n",
              "4   57    0   0     120   354    0  ...     1      0.6    2    0      2       1\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGzEEBfLOwu",
        "outputId": "60011940-d669-45d5-ff13-a5ee16942445"
      },
      "source": [
        "categorical_columns = [\"sex\", \"exng\", \"caa\", \"cp\", \"fbs\", \"restecg\", \"slp\", \"thall\"]\n",
        "continuous_columns = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
        "\n",
        "df = pd.get_dummies(heart, columns = categorical_columns)\n",
        "X = df.drop(\"output\", axis = 1)\n",
        "y = df[\"output\"]\n",
        "\n",
        "X[continuous_columns] = preprocessing.StandardScaler().fit_transform(X[continuous_columns])\n",
        "\n",
        "X_train, X_test_full, y_train, y_test_full = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_test_full, y_test_full, test_size = 0.5, random_state = 42)\n",
        "\n",
        "X_train = torch.tensor(X_train.values).float()\n",
        "X_validation = torch.tensor(X_validation.values).float()\n",
        "X_test = torch.tensor(X_test.values).float()\n",
        "X_test_full = torch.tensor(X_test_full.values).float()\n",
        "\n",
        "y_train = torch.tensor(y_train.values).float()\n",
        "y_validation = torch.tensor(y_validation.values).float()\n",
        "y_test = torch.tensor(y_test.values).float()\n",
        "y_test_full = torch.tensor(y_test_full.values).float()\n",
        "\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([212, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG5W3jPUOYIp"
      },
      "source": [
        "batch_size = 10\n",
        "train = DataLoader(TensorDataset(X_train, y_train), batch_size)\n",
        "validation = DataLoader(TensorDataset(X_validation, y_validation), batch_size)\n",
        "test = DataLoader(TensorDataset(X_test, y_test), batch_size)\n",
        "test_full = DataLoader(TensorDataset(X_test_full, y_test_full), batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYCPcp81V2Jd"
      },
      "source": [
        "# Neural Network model, train, test and benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IUGqGafTz3L"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, negative_slope=0, dropout_rate=0):\n",
        "        super(Net, self).__init__()\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.fc1 = nn.Linear(30, 64)  \n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 16)\n",
        "        self.fc4 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        leaky_relu = nn.LeakyReLU(self.negative_slope)\n",
        "        x = nn.Dropout(self.dropout_rate)(leaky_relu(self.fc1(x)))\n",
        "        x = nn.Dropout(self.dropout_rate)(leaky_relu(self.fc2(x)))\n",
        "        x = nn.Dropout(self.dropout_rate)(leaky_relu(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOSwXw_SzWDM"
      },
      "source": [
        "def train_model(model, optimizer, criterion, train, nb_epochs = 100, print_loss = False):\n",
        "    eta = 1e-1\n",
        "    print_frequency = nb_epochs / 10\n",
        "    for e in range(nb_epochs):\n",
        "        acc_loss = 0\n",
        "\n",
        "        for train_input, train_target in train:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_input.float())\n",
        "            loss = criterion(output, train_target.unsqueeze(1))\n",
        "            \n",
        "            acc_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if print_loss and e % print_frequency == 0 :\n",
        "            print(f\"Epoch {e}: Loss = {loss} \")\n",
        "    \n",
        "    return acc_loss\n",
        "\n",
        "def compute_nb_errors(model, test, criterion):\n",
        "    nb_errors = 0\n",
        "    i = 0\n",
        "    acc_loss = 0\n",
        "    for test_input, test_target in test:\n",
        "        output = model(test_input)\n",
        "        loss = criterion(output, test_target.unsqueeze(1))\n",
        "        acc_loss += loss\n",
        "        predicted_classes = output > 0.5 \n",
        "        for k in range(test_input.size(0)):\n",
        "            if test_target[k] != predicted_classes[k]:\n",
        "                nb_errors = nb_errors + 1\n",
        "    return nb_errors, acc_loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUYM7W1xe9vM"
      },
      "source": [
        "def benchmark_hyperparameters(train, test, dropout_rate=0, negative_slope=0, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=0, print_results=True):\n",
        "  betas = beta_1, beta_2\n",
        "  net = Net(negative_slope=negative_slope, dropout_rate=dropout_rate)\n",
        "  optimizer = optim.Adam(net.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "  criterion = nn.MSELoss()\n",
        "  loss_train = train_model(net, optimizer, criterion, train)\n",
        "  nb_errors, loss = compute_nb_errors(net, test, criterion)\n",
        "  accuracy = 100 * (1 - (nb_errors / len(test.dataset)))\n",
        "  if print_results:\n",
        "    print(f\"Hyperparameters value: dropout_rate={dropout_rate}, negative_slope={negative_slope}, lr={lr}, betas={betas}, weight_decay={weight_decay}\")\n",
        "    print(f\"Results: final loss {loss}, accuracy: {accuracy:.3f}\")\n",
        "    print(\"______________\")\n",
        "  return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4skaeWDWCsL"
      },
      "source": [
        "# Manual fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2LAne0ON3vK",
        "outputId": "c06ed035-5891-424a-b8e3-1e8e7f4374ea"
      },
      "source": [
        "loss, accuracy = benchmark_hyperparameters(train, test, dropout_rate=0.1, negative_slope=0.1, lr=0.0001, beta_1=0.9, beta_2=0.9, weight_decay=0, print_results=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters value: dropout_rate=0.1, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.8472055792808533, accuracy: 78.261\n",
            "______________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoNWbeDEWE5T"
      },
      "source": [
        "# Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvDCXtarlouQ"
      },
      "source": [
        "def grid_search(train, validation, test, dropout_rate_array, negative_slope_array, lr_array, beta_1_array, beta_2_array, weight_decay_array):\n",
        "  best_hyperparameters_loss = {}\n",
        "  best_hyperparameters_accuracy = {}\n",
        "  best_loss = 9999\n",
        "  best_accuracy = 0\n",
        "  results = []\n",
        "  for dropout_rate in dropout_rate_array:\n",
        "    for negative_slope in negative_slope_array:\n",
        "      for lr in lr_array:\n",
        "        for beta_1 in beta_1_array:\n",
        "          for beta_2 in beta_2_array:\n",
        "            for weight_decay in weight_decay_array:\n",
        "              loss, accuracy = benchmark_hyperparameters(train, validation, dropout_rate, negative_slope, lr, beta_1, beta_2, weight_decay)\n",
        "              iteration_dict = {}\n",
        "              iteration_dict[\"dropout_rate\"] = dropout_rate\n",
        "              iteration_dict[\"negative_slope\"] = negative_slope\n",
        "              iteration_dict[\"lr\"] = lr\n",
        "              iteration_dict[\"beta_1\"] = beta_1\n",
        "              iteration_dict[\"beta_2\"] = beta_2\n",
        "              iteration_dict[\"weight_decay\"] = weight_decay\n",
        "              iteration_dict[\"loss\"] = loss\n",
        "              iteration_dict[\"accuracy\"] = accuracy\n",
        "              results.append(iteration_dict)\n",
        "\n",
        "  for entry in results:\n",
        "    if entry[\"loss\"] < best_loss:\n",
        "      best_hyperparameters_loss[\"dropout_rate\"] = entry[\"dropout_rate\"]\n",
        "      best_hyperparameters_loss[\"negative_slope\"] = entry[\"negative_slope\"]\n",
        "      best_hyperparameters_loss[\"lr\"] = entry[\"lr\"]\n",
        "      best_hyperparameters_loss[\"beta_1\"] = entry[\"beta_1\"]\n",
        "      best_hyperparameters_loss[\"beta_2\"] = entry[\"beta_2\"]\n",
        "      best_hyperparameters_loss[\"weight_decay\"] = entry[\"weight_decay\"]\n",
        "      best_loss = entry[\"loss\"]\n",
        "    if entry[\"accuracy\"] > best_accuracy:\n",
        "      best_hyperparameters_accuracy[\"dropout_rate\"] = entry[\"dropout_rate\"]\n",
        "      best_hyperparameters_accuracy[\"negative_slope\"] = entry[\"negative_slope\"]\n",
        "      best_hyperparameters_accuracy[\"lr\"] = entry[\"lr\"]\n",
        "      best_hyperparameters_accuracy[\"beta_1\"] = entry[\"beta_1\"]\n",
        "      best_hyperparameters_accuracy[\"beta_2\"] = entry[\"beta_2\"]\n",
        "      best_hyperparameters_accuracy[\"weight_decay\"] = entry[\"weight_decay\"]\n",
        "      best_accuracy = entry[\"accuracy\"]\n",
        "\n",
        "  final_results = {}\n",
        "\n",
        "  dr_l, slope_l, lr_l, wd_l = best_hyperparameters_loss[\"dropout_rate\"], best_hyperparameters_loss[\"negative_slope\"], best_hyperparameters_loss[\"lr\"], best_hyperparameters_loss[\"weight_decay\"]\n",
        "  beta_1_l, beta_2_l = best_hyperparameters_loss[\"beta_1\"], best_hyperparameters_loss[\"beta_2\"]\n",
        "  betas_l = beta_1_l, beta_2_l\n",
        "  final_results[\"best_loss_loss\"], final_results[\"best_loss_accuracy\"] = benchmark_hyperparameters(train, test, dr_l, slope_l, lr_l, beta_1_l, beta_2_l, wd_l, print_results=True)\n",
        "\n",
        "  dr_a, slope_a, lr_a, wd_a = best_hyperparameters_accuracy[\"dropout_rate\"], best_hyperparameters_accuracy[\"negative_slope\"], best_hyperparameters_accuracy[\"lr\"], best_hyperparameters_accuracy[\"weight_decay\"]\n",
        "  beta_1_a, beta_2_a = best_hyperparameters_accuracy[\"beta_1\"], best_hyperparameters_accuracy[\"beta_2\"]\n",
        "  betas_a = beta_1_a, beta_2_a\n",
        "  final_results[\"best_accuracy_loss\"], final_results[\"best_accuracy_accuracy\"] = benchmark_hyperparameters(train, test, dropout_rate=dr_a, negative_slope=slope_a, lr=lr_a, beta_1=beta_1_a, beta_2=beta_2_a, weight_decay=wd_a, print_results=True)\n",
        "\n",
        "\n",
        "  best_loss_loss, best_loss_accuracy, best_accuracy_loss, best_accuracy_accuracy = final_results[\"best_loss_loss\"], final_results[\"best_loss_accuracy\"], final_results[\"best_accuracy_loss\"], final_results[\"best_accuracy_accuracy\"]\n",
        "  print(f\"\\n \\n \\n \\n====================================\")\n",
        "  print(f\"Best loss hyperparameter values: dropout_rate={dr_l},\"\n",
        "        f\"negative_slope={slope_l}, lr={lr_l}\"\n",
        "        f\", betas={betas_l}, weight_decay={wd_l}\")\n",
        "  print(f\"Loss on test with best loss hyperparameters: {best_loss_loss}\")\n",
        "  print(f\"Accuracy on test with best loss hyperparameters: {best_loss_accuracy}\")\n",
        "\n",
        "  print(f\"\\n====================================\")\n",
        "  betas_accuracy = best_hyperparameters_accuracy[\"beta_1\"], best_hyperparameters_accuracy[\"beta_2\"]\n",
        "  dr_a, slope_a, lr_a, wd_a = best_hyperparameters_accuracy[\"dropout_rate\"], best_hyperparameters_accuracy[\"negative_slope\"], best_hyperparameters_accuracy[\"lr\"], best_hyperparameters_accuracy[\"weight_decay\"]\n",
        "  print(f\"Best accuracy hyperparameter values: dropout_rate={dr_a},\"\\\n",
        "        f\"negative_slope={slope_a}, lr={lr_a}\"\\\n",
        "        f\", betas={betas_a}, weight_decay={wd_a}\")\n",
        "  print(f\"Loss on test with best accuracy hyperparameters: {best_accuracy_loss}\")\n",
        "  print(f\"Accuracy on test with best accuracy hyperparameters: {best_accuracy_accuracy}\")\n",
        "\n",
        "  return pd.DataFrame(results), final_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WexhQeGgpmdm",
        "outputId": "f8ffd822-b374-4b42-a1a7-e603de7eebca"
      },
      "source": [
        "dropout_rate_array= [0, 0.15, 0.3] \n",
        "negative_slope_array = [0, 0.1, 0.2] \n",
        "lr_array = [1e-5, 1e-4, 1e-3] \n",
        "beta_1_array = [0.5, 0.75, 0.9] \n",
        "beta_2_array = [0.9, 0.999, 0.99999]\n",
        "weight_decay_array = [0, 1e-7, 1e-5] \n",
        "\n",
        "results, final_benchmark = grid_search(train, validation, test, dropout_rate_array, negative_slope_array, lr_array, beta_1_array, beta_2_array, weight_decay_array)\n",
        "results.to_csv(\"benchmark.csv\")\n",
        "files.download(\"benchmark.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1507445573806763, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1516939401626587, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1139764785766602, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.061532974243164, accuracy: 71.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0606952905654907, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1619325876235962, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1333236694335938, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.0517988204956055, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0208561420440674, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.1151695251464844, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1496751308441162, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1372932195663452, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.125719428062439, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1341946125030518, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1592355966567993, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0917840003967285, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.0751601457595825, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1248846054077148, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1972272396087646, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1273239850997925, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1976797580718994, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.0681252479553223, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.100541353225708, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1142834424972534, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0829668045043945, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1388412714004517, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.9919938445091248, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.3418441414833069, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.32888075709342957, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.2760951817035675, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.23332509398460388, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.30476242303848267, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.27489668130874634, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.31225091218948364, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3230993151664734, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.24877774715423584, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.32720470428466797, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.28038647770881653, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3374860882759094, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.25493961572647095, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.23755647242069244, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2560441493988037, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2359534651041031, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2643478512763977, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2270902842283249, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.28652435541152954, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3199116885662079, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3035522699356079, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.2796631157398224, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.24718797206878662, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.29776111245155334, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3054937422275543, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.29220902919769287, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.27418676018714905, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.817331075668335, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3373609185218811, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4500122368335724, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.28844863176345825, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.28420668840408325, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.32136771082878113, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.321599543094635, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4879381060600281, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.37348222732543945, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.7343494296073914, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.38830819725990295, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4601409435272217, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.28807273507118225, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2020346224308014, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3343593180179596, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.5335148572921753, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4107210040092468, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.5848091840744019, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.6071996092796326, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.6524321436882019, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.5221281051635742, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.4329741895198822, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.23579278588294983, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.41302230954170227, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.5107453465461731, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.27930209040641785, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.46884846687316895, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1827601194381714, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1672313213348389, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.0890017747879028, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.9972796440124512, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.019711971282959, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.074940800666809, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0791088342666626, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.052689790725708, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0646262168884277, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.1878247261047363, accuracy: 46.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1625497341156006, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1881076097488403, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.0723267793655396, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0787193775177002, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1940644979476929, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.951442301273346, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.05045485496521, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.019473671913147, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1538622379302979, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.128095030784607, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.071622371673584, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.022868037223816, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0216631889343262, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0934354066848755, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0023119449615479, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.0506157875061035, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0184619426727295, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.30302849411964417, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.30318519473075867, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.29756277799606323, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.30181044340133667, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2638360559940338, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.26532480120658875, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2743949890136719, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2855144441127777, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3020074963569641, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3617474138736725, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3151088058948517, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.34183019399642944, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.35390937328338623, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2976454198360443, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.28036773204803467, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.25281521677970886, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2695687413215637, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.28213629126548767, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.27448976039886475, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.2979690432548523, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3888512849807739, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.23985350131988525, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2968953549861908, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2621617317199707, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.29212486743927, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.31838035583496094, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2578314244747162, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.3315448462963104, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4950523376464844, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.41426342725753784, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.5657163858413696, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3561105728149414, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3640032708644867, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.40917304158210754, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.35564637184143066, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2852628231048584, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3992232084274292, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4971180856227875, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4112232029438019, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.3499166965484619, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2718978524208069, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3730124235153198, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3079851269721985, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2618896961212158, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4629364013671875, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.5592785477638245, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.35984498262405396, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.43541041016578674, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3259884715080261, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3125734329223633, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2742340564727783, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2690010368824005, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.29202815890312195, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2744768559932709, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.093377947807312, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1018261909484863, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.080587387084961, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.0010285377502441, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0416204929351807, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0441205501556396, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.9009385704994202, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.054368495941162, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0165042877197266, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.082551121711731, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.132460355758667, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.115135908126831, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.0344136953353882, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.9714202880859375, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0231976509094238, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.9942334890365601, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.9229071140289307, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.9729999303817749, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1250216960906982, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1096230745315552, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1007840633392334, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.0501123666763306, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0013577938079834, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0066853761672974, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0208115577697754, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.9827378988265991, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.042272925376892, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.32815778255462646, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.30950915813446045, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3608138859272003, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.26334500312805176, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.25159570574760437, accuracy: 97.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.24892272055149078, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3217790722846985, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2551240026950836, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.26251745223999023, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.33822953701019287, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.31036698818206787, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3133818507194519, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.27459627389907837, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3012583553791046, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2983315885066986, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3570849299430847, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.29690617322921753, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.32321828603744507, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.3340194523334503, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.323195219039917, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.27629032731056213, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.2541150748729706, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2931104004383087, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2934781312942505, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2564440071582794, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2702346742153168, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.26158788800239563, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.2969686686992645, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.6840913891792297, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.20075058937072754, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.31987765431404114, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3808968663215637, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.36768674850463867, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2421777844429016, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3036816418170929, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.23302772641181946, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.4412906765937805, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4595543146133423, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.41296255588531494, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.305004358291626, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3075941205024719, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.15974393486976624, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.21498370170593262, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2635347843170166, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2423943281173706, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.4537097215652466, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3001559376716614, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.5485998392105103, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3929797410964966, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2655995488166809, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.30619171261787415, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2772158682346344, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.12566371262073517, accuracy: 97.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4481865167617798, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1662126779556274, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1933461427688599, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.2130122184753418, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.1386088132858276, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.234620451927185, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.184592604637146, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.132411241531372, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.146910309791565, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.15178382396698, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.154407262802124, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1679527759552002, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1414616107940674, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.1757426261901855, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0875141620635986, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1433545351028442, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.208261251449585, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.139459490776062, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1154780387878418, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1920068264007568, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.201154112815857, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1798290014266968, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.1793670654296875, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1059730052947998, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.2013719081878662, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1873725652694702, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.2090890407562256, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.2053238153457642, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.4177786707878113, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.33846116065979004, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4295971393585205, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.2824409604072571, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.24408061802387238, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4664977490901947, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.37244975566864014, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2233109027147293, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.25504085421562195, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.29625576734542847, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.492043137550354, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.40191346406936646, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.4440218508243561, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.35532230138778687, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.37599262595176697, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2802002727985382, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3010260760784149, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3301350474357605, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.4359814524650574, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4150828719139099, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3380312919616699, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3902837634086609, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.36076316237449646, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.31494513154029846, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.427019864320755, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.27569565176963806, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4052562415599823, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.5421789288520813, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.47798606753349304, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.18571485579013824, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.5274206399917603, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3226264715194702, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.574349582195282, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3476807475090027, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4062131643295288, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.1980566382408142, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.19861572980880737, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.2972366213798523, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.5415804386138916, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.5809345841407776, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.4533146619796753, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.43660756945610046, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.5462418794631958, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.41418492794036865, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3998594284057617, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.6890295743942261, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3878108859062195, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4987338185310364, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.7014687657356262, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.23609109222888947, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3154226839542389, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.35297825932502747, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4435211718082428, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4170491397380829, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1854844093322754, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1686058044433594, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.158024549484253, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.1034611463546753, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1403834819793701, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1445631980895996, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1284679174423218, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1329673528671265, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1456375122070312, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.182175874710083, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.110638976097107, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1349387168884277, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.2159966230392456, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1045033931732178, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0935121774673462, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1756795644760132, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.137526273727417, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1470742225646973, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1800999641418457, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1295421123504639, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1713141202926636, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.1260924339294434, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1263617277145386, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.143967866897583, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1181905269622803, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1282488107681274, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0694212913513184, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.37135082483291626, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3384217619895935, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3884865343570709, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.3096190392971039, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.29515689611434937, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.42614075541496277, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.2585989534854889, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.36540770530700684, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2711133360862732, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3157585859298706, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3607892394065857, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3761329650878906, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.2941890358924866, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.41744765639305115, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2875003218650818, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3721140921115875, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3789983093738556, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2529458999633789, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.46901461482048035, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.41054099798202515, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.2987697124481201, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.37738317251205444, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.35602253675460815, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3705752491950989, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3375953435897827, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3109082579612732, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3704157769680023, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.5247045159339905, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.11871787160634995, accuracy: 97.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.5075523853302002, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.3476858139038086, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.22775790095329285, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.30566275119781494, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.47109055519104004, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.37130656838417053, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.6556208729743958, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.2402634471654892, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.23733285069465637, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.18495047092437744, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.412528395652771, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2735472023487091, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3709412217140198, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.294609010219574, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4211971163749695, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2773749828338623, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.4636561870574951, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4136246144771576, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.45230698585510254, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3051756024360657, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.6981493830680847, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4370279610157013, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3025084137916565, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3951602876186371, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3641110956668854, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1529799699783325, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1293448209762573, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1493544578552246, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.1469120979309082, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1106010675430298, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0928839445114136, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1092047691345215, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.0988909006118774, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1827818155288696, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.119944453239441, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.129683017730713, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1795728206634521, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.1294007301330566, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0821447372436523, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1024169921875, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1224961280822754, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.0068243741989136, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0327239036560059, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.107966423034668, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.129052996635437, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1699540615081787, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.1403660774230957, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0673933029174805, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.0875134468078613, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0654091835021973, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.9837979078292847, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.0685826539993286, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.459733784198761, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3593958616256714, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.397081196308136, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.32063859701156616, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2932199537754059, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.34179115295410156, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3915214240550995, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3305080235004425, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3315190076828003, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.32154718041419983, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.2893502712249756, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.47800296545028687, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.355427086353302, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3115566670894623, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.25086167454719543, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3489622473716736, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.42899537086486816, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.293266236782074, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.3049410283565521, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.39151906967163086, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.2607404291629791, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.20635557174682617, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.368647038936615, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4102814793586731, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.24745018780231476, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2960546314716339, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.19993920624256134, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.41976693272590637, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3134533762931824, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.32925254106521606, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.28516533970832825, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.27478301525115967, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3069364130496979, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.12435441464185715, accuracy: 97.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.27640095353126526, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.18835557997226715, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3274148106575012, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.37099573016166687, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.2211838960647583, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.5314525961875916, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.42145848274230957, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.27903521060943604, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3342641294002533, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.34625762701034546, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.48916834592819214, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.3100910782814026, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.5080863237380981, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.20375528931617737, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.36497047543525696, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.33618879318237305, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3898073732852936, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.34513673186302185, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.32979708909988403, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3179648816585541, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.2047910690307617, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.2362302541732788, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.2187178134918213, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.164169430732727, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1820327043533325, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1937003135681152, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1784321069717407, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.2124700546264648, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.184786319732666, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.2155131101608276, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.2329697608947754, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.204240083694458, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.1824579238891602, accuracy: 71.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.2274645566940308, accuracy: 51.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1927120685577393, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.2433650493621826, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1801507472991943, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1867457628250122, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1925073862075806, accuracy: 51.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.2331366539001465, accuracy: 48.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.2317781448364258, accuracy: 46.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.185752034187317, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.2342431545257568, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.199961543083191, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1914846897125244, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1794617176055908, accuracy: 60.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1850847005844116, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.4537365138530731, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.41112589836120605, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.6026118993759155, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.25470152497291565, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3394812345504761, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4215758740901947, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3863753080368042, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3876352310180664, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.5040464997291565, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3951852321624756, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.5243883728981018, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4140738844871521, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.427081823348999, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.2997976541519165, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.5333597660064697, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3315683603286743, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4034634232521057, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3740004301071167, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.3875141739845276, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.34533801674842834, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.44625699520111084, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3080001175403595, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.4985297918319702, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.36527788639068604, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3899686932563782, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3789305090904236, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3628039062023163, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.47356897592544556, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3679415285587311, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3954789340496063, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.4288283884525299, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.5606797337532043, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4403129816055298, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.36269333958625793, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.30467489361763, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.21776466071605682, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.4190921485424042, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.46266114711761475, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3623617887496948, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.26467591524124146, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3532230257987976, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.20408108830451965, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.47522085905075073, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.37287330627441406, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4227507412433624, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.42654818296432495, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.24281379580497742, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.40417349338531494, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.31245797872543335, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.43679279088974, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.2882040739059448, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.5060106515884399, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.30813249945640564, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.44858017563819885, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1710450649261475, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.221591591835022, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.218123435974121, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.218119740486145, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1568903923034668, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.23549222946167, accuracy: 46.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1544814109802246, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.2133398056030273, accuracy: 51.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1020944118499756, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.193131685256958, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.169588565826416, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1966270208358765, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.1714428663253784, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1577825546264648, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.2107243537902832, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.2066872119903564, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1632717847824097, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1949409246444702, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1355648040771484, accuracy: 71.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.2176094055175781, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.2009007930755615, accuracy: 64.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.1843832731246948, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.229556679725647, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.2065904140472412, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.2023110389709473, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1680762767791748, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.140437126159668, accuracy: 77.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.4162140488624573, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.32625624537467957, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4594295024871826, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.37624040246009827, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3313564658164978, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.3680996894836426, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3497716784477234, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4260912239551544, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.407419890165329, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.33301717042922974, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4176844358444214, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.41235870122909546, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.5115747451782227, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.4075724184513092, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.38924655318260193, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3878104090690613, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.5142802000045776, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.43101829290390015, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.4581563174724579, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.4462566673755646, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3735867440700531, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.42998307943344116, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.493554025888443, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.39588120579719543, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.40521642565727234, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2809092104434967, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3396148979663849, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.45983052253723145, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.37925708293914795, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.37288668751716614, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.4775337874889374, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.24397125840187073, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.22405576705932617, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.29467493295669556, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2664412558078766, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3756031394004822, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.3794531524181366, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.39330485463142395, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.15711328387260437, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.4580601751804352, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.49017658829689026, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.39588552713394165, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.436369925737381, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.4231942296028137, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.43173274397850037, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.37920433282852173, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.24690204858779907, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.34575092792510986, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.4163531959056854, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.5043856501579285, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.38903698325157166, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.45392903685569763, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.5338497161865234, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.1, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3600810468196869, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 1.1876388788223267, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.2151591777801514, accuracy: 62.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1552424430847168, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 1.0846978425979614, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1769853830337524, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.169333577156067, accuracy: 75.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 1.165850043296814, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1309309005737305, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1944987773895264, accuracy: 46.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 1.1385934352874756, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1747255325317383, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.1952732801437378, accuracy: 53.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 1.187928557395935, accuracy: 55.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.0682488679885864, accuracy: 82.222\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1488537788391113, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 1.0929394960403442, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.171044945716858, accuracy: 66.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1487048864364624, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 1.1848227977752686, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.1608916521072388, accuracy: 80.000\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 1.144834041595459, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 1.1929709911346436, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 1.1495927572250366, accuracy: 57.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 1.1298801898956299, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 1.1802682876586914, accuracy: 68.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 1.1893517971038818, accuracy: 71.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=1e-05, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 1.1659642457962036, accuracy: 73.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.3504956066608429, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3701710104942322, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.4085516631603241, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.47037041187286377, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.43214094638824463, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.33704644441604614, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.331997275352478, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.28997695446014404, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.3811579644680023, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.39174771308898926, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.46125757694244385, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.49747681617736816, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.37726378440856934, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3506892919540405, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.348384290933609, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.3392997086048126, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.46790745854377747, accuracy: 84.444\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.33559274673461914, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.45023152232170105, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.3739263117313385, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3010900616645813, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3324580490589142, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.5142876505851746, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4603172838687897, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.29322874546051025, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3428388833999634, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.0001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.2904359698295593, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=0\n",
            "Results: final loss 0.2877902090549469, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.1272968202829361, accuracy: 97.778\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.370595246553421, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=0\n",
            "Results: final loss 0.334373414516449, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.4548889994621277, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.18361131846904755, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=0\n",
            "Results: final loss 0.37405484914779663, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.2821938395500183, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.5, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.4292008578777313, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=0\n",
            "Results: final loss 0.27723339200019836, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.24174340069293976, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.2949454188346863, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=0\n",
            "Results: final loss 0.29810312390327454, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.3880542814731598, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.36481088399887085, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=0\n",
            "Results: final loss 0.43608933687210083, accuracy: 91.111\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.3573569059371948, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.75, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.1866631656885147, accuracy: 95.556\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=0\n",
            "Results: final loss 0.44522178173065186, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-07\n",
            "Results: final loss 0.5120058059692383, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.9), weight_decay=1e-05\n",
            "Results: final loss 0.3091975152492523, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=0\n",
            "Results: final loss 0.3151501715183258, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.23443976044654846, accuracy: 93.333\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.999), weight_decay=1e-05\n",
            "Results: final loss 0.4597555696964264, accuracy: 86.667\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=0\n",
            "Results: final loss 0.36296728253364563, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-07\n",
            "Results: final loss 0.41519272327423096, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.3, negative_slope=0.2, lr=0.001, betas=(0.9, 0.99999), weight_decay=1e-05\n",
            "Results: final loss 0.41769543290138245, accuracy: 88.889\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0.15, negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Results: final loss 1.0766433477401733, accuracy: 78.261\n",
            "______________\n",
            "Hyperparameters value: dropout_rate=0, negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Results: final loss 0.7981595993041992, accuracy: 82.609\n",
            "______________\n",
            "\n",
            " \n",
            " \n",
            " \n",
            "====================================\n",
            "Best loss hyperparameter values: dropout_rate=0.15,negative_slope=0.1, lr=0.001, betas=(0.5, 0.9), weight_decay=1e-07\n",
            "Loss on test with best loss hyperparameters: 1.0766433477401733\n",
            "Accuracy on test with best loss hyperparameters: 78.26086956521739\n",
            "\n",
            "====================================\n",
            "Best accuracy hyperparameter values: dropout_rate=0,negative_slope=0.2, lr=0.0001, betas=(0.5, 0.999), weight_decay=1e-07\n",
            "Loss on test with best accuracy hyperparameters: 0.7981595993041992\n",
            "Accuracy on test with best accuracy hyperparameters: 82.6086956521739\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3a2c2072-7169-461c-adc7-35db56c3a917\", \"benchmark.csv\", 51820)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zKSscwrjO7IP",
        "outputId": "c6491a97-3227-468b-fdcc-9dbd993b0645"
      },
      "source": [
        "results_plot = results[[\"lr\", \"dropout_rate\", \"accuracy\"]].groupby([\"lr\", \"dropout_rate\"]).mean().reset_index()\n",
        "results_plot = results_plot.pivot(\"lr\", \"dropout_rate\", \"accuracy\")\n",
        "sns.heatmap(results_plot, annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f84f6f38bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjAtOFTiWJ8s"
      },
      "source": [
        "# Basic Random search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voi8_3Ah9h04"
      },
      "source": [
        "def random_search(train, validation, test, initialization_vector, weight_vector, nb_iter):\n",
        "  vector = initialization_vector\n",
        "  gamma_list = [5, 1, 5e-1, 1e-1, 5e-2, 1e-2, 0, -1e-2, -5e-2, -1e-1, -5e-1, -1, -5]\n",
        "  for i in range(nb_iter):\n",
        "    random_vector = torch.rand(6) * 2 - 1\n",
        "    random_vector_scaled = random_vector * weight_vector\n",
        "    best_loss_iter = 999\n",
        "    best_vector_iter = vector\n",
        "    best_gamma_iter = 42\n",
        "    best_accuracy = 0\n",
        "    for gamma in gamma_list:\n",
        "      vector_gamma = vector - gamma*random_vector_scaled\n",
        "      vector_gamma[vector_gamma < 0] = 0\n",
        "      vector_gamma[vector_gamma > 1] = 0.9999999\n",
        "      loss, accuracy = benchmark_hyperparameters(train, validation, vector_gamma[0], vector_gamma[1], vector_gamma[2], vector_gamma[3], vector_gamma[4], vector_gamma[5], print_results=False)\n",
        "      if loss < best_loss_iter:\n",
        "        best_loss_iter = loss\n",
        "        best_vector_iter = vector_gamma\n",
        "        best_gamma_iter = gamma\n",
        "        best_accuracy_iter = accuracy\n",
        "    vector = best_vector_iter\n",
        "    print(f\"At iteration {i}, best vector is {vector} and loss is {best_loss_iter} with gamma factor {best_gamma_iter} and accuracy {best_accuracy_iter} \")\n",
        "  print(f\"=============\")\n",
        "\n",
        "\n",
        "  loss_test, accuracy_test = benchmark_hyperparameters(train, test, vector[0], vector[1], vector[2], vector[3], vector[4], vector[5])\n",
        "  print(f\"At the end of the random search, best vector is {vector} with associated loss {loss_test} and associated accuracy {accuracy_test}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOoi1iQQq72b",
        "outputId": "295bd4a0-0339-4dec-b22c-60c3d9016196"
      },
      "source": [
        "pytorch_initialization = torch.Tensor([0, 0, 1e-3, 0.9, 0.999, 0])\n",
        "weights = torch.Tensor([1e-1, 1e-1, 1e-3, 1e-1, 1e-3, 1e-8])\n",
        "random_search(train, validation, test, pytorch_initialization, weights, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At iteration 0, best vector is tensor([0.0000e+00, 0.0000e+00, 1.0162e-03, 8.9776e-01, 9.9903e-01, 1.8817e-10]) and loss is 0.16072429716587067 with gamma factor 0.05 and accuracy 95.55555555555556 \n",
            "At iteration 1, best vector is tensor([0.0000e+00, 2.3367e-01, 5.4493e-03, 1.0000e+00, 1.0000e+00, 1.1975e-08]) and loss is 0.20000000298023224 with gamma factor 5 and accuracy 95.55555555555556 \n",
            "At iteration 2, best vector is tensor([4.4822e-02, 2.3641e-01, 6.3947e-03, 1.0000e+00, 1.0000e+00, 1.4262e-08]) and loss is 0.3181741535663605 with gamma factor 1 and accuracy 93.33333333333333 \n",
            "At iteration 3, best vector is tensor([0.0000e+00, 1.6065e-01, 5.9214e-03, 1.0000e+00, 9.9963e-01, 1.1310e-08]) and loss is 0.10192974656820297 with gamma factor 1 and accuracy 97.77777777777777 \n",
            "At iteration 4, best vector is tensor([3.5221e-02, 1.4073e-01, 6.0675e-03, 9.6481e-01, 9.9928e-01, 7.4547e-09]) and loss is 0.2909809648990631 with gamma factor 0.5 and accuracy 93.33333333333333 \n",
            "At iteration 5, best vector is tensor([3.4919e-02, 1.4098e-01, 6.0763e-03, 9.6531e-01, 9.9928e-01, 7.3652e-09]) and loss is 0.19463452696800232 with gamma factor -0.01 and accuracy 95.55555555555556 \n",
            "At iteration 6, best vector is tensor([0.0000e+00, 0.0000e+00, 2.9121e-03, 4.9554e-01, 9.9964e-01, 1.8937e-08]) and loss is 0.21903663873672485 with gamma factor -5 and accuracy 95.55555555555556 \n",
            "At iteration 7, best vector is tensor([7.5557e-03, 0.0000e+00, 2.7089e-03, 4.5456e-01, 9.9982e-01, 1.6959e-08]) and loss is 0.13049639761447906 with gamma factor -0.5 and accuracy 97.77777777777777 \n",
            "At iteration 8, best vector is tensor([0.0000, 0.0000, 0.0038, 0.4573, 1.0000, 0.0000]) and loss is 0.2872738838195801 with gamma factor -5 and accuracy 93.33333333333333 \n",
            "At iteration 9, best vector is tensor([0.0000, 0.0000, 0.0038, 0.4603, 1.0000, 0.0000]) and loss is 0.1970234513282776 with gamma factor -0.05 and accuracy 93.33333333333333 \n",
            "=============\n",
            "Hyperparameters value: dropout_rate=0.0, negative_slope=0.0, lr=0.0037987076211720705, betas=(tensor(0.4603), tensor(1.0000)), weight_decay=0.0\n",
            "Results: final loss 0.9740747213363647, accuracy: 78.261\n",
            "______________\n",
            "At the end of the random search, best vector is tensor([0.0000, 0.0000, 0.0038, 0.4603, 1.0000, 0.0000]) with associated loss 0.9740747213363647 and associated accuracy 78.26086956521739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtQXpvWZvTWG",
        "outputId": "9c608e53-0cad-4b3f-8b2f-561d5c92518b"
      },
      "source": [
        "median_initialization = torch.Tensor([0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
        "weight_median = torch.Tensor([1, 1, 1, 1, 1, 1])\n",
        "random_search(train, validation, test, median_initialization, weight_median, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At iteration 0, best vector is tensor([0.0000, 0.8042, 1.0000, 1.0000, 1.0000, 0.5063]) and loss is 0.4253215491771698 with gamma factor -1 and accuracy 88.88888888888889 \n",
            "At iteration 1, best vector is tensor([0.0000, 0.7589, 1.0000, 1.0000, 1.0000, 0.4899]) and loss is 0.4859849512577057 with gamma factor 0.1 and accuracy 86.66666666666667 \n",
            "At iteration 2, best vector is tensor([0.0000, 0.8004, 1.0000, 1.0000, 1.0000, 0.5122]) and loss is 0.5224184989929199 with gamma factor 0.05 and accuracy 86.66666666666667 \n",
            "At iteration 3, best vector is tensor([0.0206, 1.0000, 1.0000, 1.0000, 1.0000, 0.4224]) and loss is 0.7671244144439697 with gamma factor 1 and accuracy 77.77777777777779 \n",
            "At iteration 4, best vector is tensor([0.0206, 1.0000, 1.0000, 1.0000, 1.0000, 0.4224]) and loss is 0.7678804993629456 with gamma factor 0 and accuracy 80.0 \n",
            "At iteration 5, best vector is tensor([0.0300, 1.0000, 1.0000, 1.0000, 1.0000, 0.4140]) and loss is 0.6842760443687439 with gamma factor -0.01 and accuracy 82.22222222222221 \n",
            "At iteration 6, best vector is tensor([0.0542, 0.9971, 0.9621, 1.0000, 1.0000, 0.3708]) and loss is 0.43423131108283997 with gamma factor 0.05 and accuracy 86.66666666666667 \n",
            "At iteration 7, best vector is tensor([0.0542, 0.9971, 0.9621, 1.0000, 1.0000, 0.3708]) and loss is 0.6044912338256836 with gamma factor 0 and accuracy 84.44444444444444 \n",
            "At iteration 8, best vector is tensor([0.0503, 0.9947, 0.9684, 1.0000, 0.9982, 0.3806]) and loss is 1.129654884338379 with gamma factor 0.01 and accuracy 75.55555555555556 \n",
            "At iteration 9, best vector is tensor([0.0919, 0.9967, 0.9848, 1.0000, 1.0000, 0.3732]) and loss is 0.9511500597000122 with gamma factor 0.05 and accuracy 77.77777777777779 \n",
            "At iteration 10, best vector is tensor([0.1499, 1.0000, 0.6443, 1.0000, 1.0000, 0.7032]) and loss is 0.641021728515625 with gamma factor -0.5 and accuracy 82.22222222222221 \n",
            "At iteration 11, best vector is tensor([0.1534, 1.0000, 0.6463, 1.0000, 1.0000, 0.6897]) and loss is 0.7142928242683411 with gamma factor -0.05 and accuracy 80.0 \n",
            "At iteration 12, best vector is tensor([0.1534, 1.0000, 0.6463, 1.0000, 1.0000, 0.6897]) and loss is 0.9997524619102478 with gamma factor 0 and accuracy 68.88888888888889 \n",
            "At iteration 13, best vector is tensor([0.1629, 1.0000, 0.6466, 1.0000, 0.9990, 0.6864]) and loss is 0.7873079776763916 with gamma factor -0.01 and accuracy 71.11111111111111 \n",
            "At iteration 14, best vector is tensor([0.1629, 1.0000, 0.6466, 1.0000, 0.9990, 0.6864]) and loss is 1.0568506717681885 with gamma factor 0 and accuracy 60.0 \n",
            "At iteration 15, best vector is tensor([0.1629, 1.0000, 0.6466, 1.0000, 0.9990, 0.6864]) and loss is 0.7922210097312927 with gamma factor 0 and accuracy 73.33333333333334 \n",
            "At iteration 16, best vector is tensor([0.1024, 0.8231, 1.0000, 1.0000, 1.0000, 1.0000]) and loss is 0.589147686958313 with gamma factor -0.5 and accuracy 77.77777777777779 \n",
            "At iteration 17, best vector is tensor([0.0790, 0.7799, 1.0000, 1.0000, 1.0000, 0.9650]) and loss is 0.5039251446723938 with gamma factor 0.05 and accuracy 82.22222222222221 \n",
            "At iteration 18, best vector is tensor([0.0000, 0.6101, 1.0000, 1.0000, 1.0000, 0.6688]) and loss is 0.3860459327697754 with gamma factor -0.5 and accuracy 88.88888888888889 \n",
            "At iteration 19, best vector is tensor([0.0000, 0.6101, 1.0000, 1.0000, 1.0000, 0.6688]) and loss is 0.7258787751197815 with gamma factor 0 and accuracy 75.55555555555556 \n",
            "=============\n",
            "Hyperparameters value: dropout_rate=0.0, negative_slope=0.6100795865058899, lr=0.9999998807907104, betas=(tensor(1.0000), tensor(1.0000)), weight_decay=0.6687597632408142\n",
            "Results: final loss 1.0111366510391235, accuracy: 71.739\n",
            "______________\n",
            "At the end of the random search, best vector is tensor([0.0000, 0.6101, 1.0000, 1.0000, 1.0000, 0.6688]) with associated loss 1.0111366510391235 and associated accuracy 71.73913043478262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJvux6ctWUsp"
      },
      "source": [
        "# Extended random search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5U3UQ45OKuU"
      },
      "source": [
        "def generate_random_iterate(old_iterate):\n",
        "  new_iterate = [1, 1, 1, 1, 1, 1]\n",
        "  for (i, v) in enumerate(old_iterate):\n",
        "    increase = rd.choice([True, False])\n",
        "    value = np.random.uniform(v, 1)\n",
        "    if increase:\n",
        "      value = 1/value\n",
        "    new_iterate[i] = v*value\n",
        "  return (new_iterate, benchmark_hyperparameters(*new_iterate)[0])\n",
        "\n",
        "def random_search_tree(trees=3, branches=5, n_iter=50, gaussian_random_variations=False):\n",
        "  # All parameters stay between 0 and 1\n",
        "  # weights = torch.Tensor([1e-1, 1e-1, 1e-3, 1e-1, 1e-3, 1e-8])\n",
        "  rd_vectors = list([[0.0001, 0.0001, 1e-3, 0.9, 0.999, 0.0001] for i in range(trees)])\n",
        "  # rd_vectors = list([np.random.uniform(0, 1, 6) for i in range(trees)])\n",
        "  init_pop = list(map(lambda x: (x, benchmark_hyperparameters(*x)[0]), rd_vectors))\n",
        "  pop = sorted(init_pop, key=lambda tup: tup[1])\n",
        "  best_iterate = pop[0]\n",
        "  for _ in range(n_iter):\n",
        "    print(\"Best iterate : {}\".format(best_iterate))\n",
        "    pool = []\n",
        "    for i in range(trees):\n",
        "      pool.append(pop[i])\n",
        "      for j in range(branches):\n",
        "        pool.append(generate_random_iterate(pop[i][0]))\n",
        "    pop = sorted(pool, key=lambda tup: tup[1])[:trees]\n",
        "    best_iterate = pop[0]\n",
        "    print(\"New population : {}\".format(pop))\n",
        "  return best_iterate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cZtcWteOOiw"
      },
      "source": [
        "def generate_random_crossover(v1, v2):\n",
        "  new_iterate = []\n",
        "  for i in range(len(v1)):\n",
        "    get_first = rd.choice([True, False])\n",
        "    if get_first:\n",
        "      new_iterate.append(v1[i])\n",
        "    else:\n",
        "      new_iterate.append(v2[i])\n",
        "  old_iterate = new_iterate.copy()\n",
        "  for (i, v) in enumerate(old_iterate):\n",
        "    nudge = rd.choice([True, False])\n",
        "    if nudge:\n",
        "      increase = rd.choice([True, False])\n",
        "      value = np.random.uniform(v, 1)\n",
        "      if increase:\n",
        "        value = 1/value\n",
        "      new_iterate[i] = v*value\n",
        "  return (new_iterate, benchmark_hyperparameters(*new_iterate)[0])\n",
        "\n",
        "def random_search_genetic(pop_size=10, n_iter=50, gaussian_random_variations=False):\n",
        "  # All parameters stay between 0 and 1\n",
        "  rd_vectors = list([[0.0001*(i+1), 0.0001*(i+1), 1e-3*(i+1), 0.9/(i+1), 0.999/(i+1), 0.0001*(i+1)] for i in range(pop_size)])\n",
        "  init_pop = list(map(lambda x: (x, benchmark_hyperparameters(*x)[0]), rd_vectors))\n",
        "  pop = sorted(init_pop, key=lambda tup: tup[1])\n",
        "  best_iterate = pop[0]\n",
        "  for _ in range(n_iter):\n",
        "    print(\"Best iterate : {}\".format(best_iterate))\n",
        "    pool = []\n",
        "    for i in range(pop_size):\n",
        "      pool.append(pop[i])\n",
        "      for j in range(pop_size)[i+1:]:\n",
        "        pool.append(generate_random_crossover(pop[i][0], pop[j][0]))\n",
        "    pop = sorted(pool, key=lambda tup: tup[1])[:pop_size]\n",
        "    best_iterate = pop[0]\n",
        "    print(\"New population : {}\".format(pop))\n",
        "  return best_iterate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xW_ubxoWY8q"
      },
      "source": [
        "# Benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpmr2rReFpnQ"
      },
      "source": [
        "def benchmark(train, test, dropout_rate=0, negative_slope=0, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=0):\n",
        "  array_loss = np.array([])\n",
        "  array_acc = np.array([])\n",
        "  for i in range(10):\n",
        "    loss, acc = benchmark_hyperparameters(train, test, print_results = False)\n",
        "    array_loss = np.append(array_loss, loss)\n",
        "    array_acc = np.append(array_acc, acc)\n",
        "\n",
        "  print(f\"Average loss is {np.mean(array_loss)} and standard deviation is {np.std(array_loss)}\")\n",
        "  print(f\"Average accuracy is {np.mean(array_acc)} and standard deviation is {np.std(array_acc)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXCxX5K9GP4G",
        "outputId": "f9534624-9536-4a7d-ec5b-971731745a04"
      },
      "source": [
        "manual_picking = [0.1, 0.1, 0.0001, 0.9, 0.9, 0]\n",
        "print(f\"Manual picking\")\n",
        "benchmark(train, test, manual_picking[0], manual_picking[1], manual_picking[2], manual_picking[3], manual_picking[4], manual_picking[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"Grid search loss\")\n",
        "grid_l = [0.15, 0.1, 0.001, 0.5, 0.9, 1e-7]\n",
        "benchmark(train, test, grid_l[0], grid_l[1], grid_l[2], grid_l[3], grid_l[4], grid_l[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"Grid search acc\")\n",
        "grid_a = [0, 0.2, 0.0001, 0.5, 0.999, 1e-7]\n",
        "benchmark(train, test, grid_a[0], grid_a[1], grid_a[2], grid_a[3], grid_a[4], grid_a[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"random search initialized\")\n",
        "random_i = [0.0000, 0.0000, 0.0038, 0.4603, 1.0000, 0.0000]\n",
        "benchmark(train, test, random_i[0], random_i[1], random_i[2], random_i[3], random_i[4], random_i[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"random search median\")\n",
        "random_m = random_i = [0, 0.6101, 1, 1, 1, 0.6688]\n",
        "benchmark(train, test, random_m[0], random_m[1], random_m[2], random_m[3], random_m[4], random_m[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"Method 1\")\n",
        "method_1 = [0.02461611955754215, 0.00013488036951462187, 0.0031084072969009783, 0.852471111047721, 0.9998966666366723, 1.3207879591899265e-05]\n",
        "benchmark(train, test, method_1[0], method_1[1], method_1[2], method_1[3], method_1[4], method_1[5])\n",
        "\n",
        "print(f\"==========\")\n",
        "print(f\"Method 2\")\n",
        "method_2 = [0.0004, 6.571766206276303e-06, 0.0005985845472056023, 0.003034251791715318, 0.9990298241543801, 0.00017028287280631955]\n",
        "benchmark(train, test, method_2[0], method_2[1], method_2[2], method_2[3], method_2[4], method_2[5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manual picking\n",
            "Average loss is 1.1761067867279054 and standard deviation is 0.09024718126845478\n",
            "Average accuracy is 73.91304347826087 and standard deviation is 3.2244341248242\n",
            "==========\n",
            "Grid search loss\n",
            "Average loss is 1.1618168234825135 and standard deviation is 0.09873344238799109\n",
            "Average accuracy is 74.1304347826087 and standard deviation is 2.9886363227972885\n",
            "==========\n",
            "Grid search acc\n",
            "Average loss is 1.2277533531188964 and standard deviation is 0.14925032769005642\n",
            "Average accuracy is 71.95652173913045 and standard deviation is 4.175950589630118\n",
            "==========\n",
            "random search initialized\n",
            "Average loss is 1.1247656106948853 and standard deviation is 0.06524186661569115\n",
            "Average accuracy is 74.78260869565217 and standard deviation is 1.4420107784153946\n",
            "==========\n",
            "random search median\n",
            "Average loss is 1.1488218307495117 and standard deviation is 0.14548331255511177\n",
            "Average accuracy is 74.34782608695653 and standard deviation is 3.6115755925730744\n",
            "==========\n",
            "Method 1\n",
            "Average loss is 1.1771810412406922 and standard deviation is 0.11209744934326435\n",
            "Average accuracy is 73.2608695652174 and standard deviation is 3.644142307443525\n",
            "==========\n",
            "Method 2\n",
            "Average loss is 1.2054585218429565 and standard deviation is 0.09650960722242694\n",
            "Average accuracy is 72.3913043478261 and standard deviation is 2.924700879798631\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}